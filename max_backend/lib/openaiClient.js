/**
 * lib/openaiClient.js
 * Client OpenAI centralis√© pour M.A.X. MVP1
 *
 * Fonctionnalit√©s:
 * - Appels OpenAI GPT-4o-mini (mod√®le simple)
 * - Appels OpenAI GPT-4o (mod√®le complexe)
 * - Support streaming (SSE)
 * - Support messages avec historique
 * - Gestion erreurs et retry
 */

import OpenAI from 'openai';
import 'dotenv/config';
import { getTenantMemory } from './maxLogger.js';
import { getLocalMemory } from './memoryFallback.js';

// Configuration depuis .env
const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
const MODEL_SIMPLE = process.env.AI_MODEL_SIMPLE || 'gpt-4o-mini';
const MODEL_COMPLEX = process.env.AI_MODEL_COMPLEX || 'gpt-4o';
const MAX_RETRY_ATTEMPTS = parseInt(process.env.MAX_RETRY_ATTEMPTS || '3', 10);
const REQUEST_TIMEOUT_MS = parseInt(process.env.MAX_REQUEST_TIMEOUT_MS || '45000', 10);

if (!OPENAI_API_KEY) {
  console.error('[OPENAI_CLIENT] ‚ùå OPENAI_API_KEY manquante dans .env');
  throw new Error('OPENAI_API_KEY is required');
}

// Instance OpenAI
const openai = new OpenAI({
  apiKey: OPENAI_API_KEY,
  timeout: REQUEST_TIMEOUT_MS,
  maxRetries: MAX_RETRY_ATTEMPTS
});

/**
 * Prompts syst√®me pour les 3 modes M.A.X.
 */
const SYSTEM_PROMPTS = {
  auto: `Tu es M.A.X., un assistant IA pour la gestion de leads CRM (MaCr√©a).

**Mode AUTO** : Tu PROPOSES des actions concr√®tes (cr√©er lead, envoyer WhatsApp, mettre √† jour statut, etc.), mais tu N'EX√âCUTES JAMAIS directement. Tu attends TOUJOURS la confirmation de l'utilisateur avant toute action.

Fonctionnalit√©s disponibles :
- Analyse de fichiers CSV/PDF/DOCX upload√©s
- Proposition d'actions sur les leads (cr√©ation, mise √† jour, enrichissement)
- Suggestion d'envoi de messages WhatsApp
- R√©sum√© et analyse de donn√©es

Tu DOIS :
- √ätre concis et professionnel
- Proposer des actions claires avec leurs cons√©quences
- Demander confirmation avant toute action critique
- Formater les r√©ponses en markdown

Tu NE DOIS PAS :
- Ex√©cuter d'actions sans confirmation
- Inventer des donn√©es
- Faire des suppositions non bas√©es sur les donn√©es fournies`,

  assist: `Tu es M.A.X., un assistant IA pour la gestion de leads CRM (MaCr√©a).

**Mode ASSIST√â** : Tu PROPOSES uniquement des recommandations. Tu n'ex√©cutes RIEN et tu ne sugg√®res m√™me pas d'ex√©cuter. Tu donnes des conseils sur ce qui POURRAIT √™tre fait.

Fonctionnalit√©s disponibles :
- Analyse de fichiers CSV/PDF/DOCX upload√©s
- Recommandations sur la gestion des leads
- Conseils sur les meilleures pratiques CRM
- Aide √† la d√©cision

Tu DOIS :
- √ätre tr√®s concis et professionnel
- Donner des recommandations claires
- Expliquer le "pourquoi" de tes suggestions
- Formater les r√©ponses en markdown

Tu NE DOIS PAS :
- Proposer d'ex√©cuter des actions
- Utiliser des formulations imp√©ratives
- Inventer des donn√©es`,

  conseil: `Tu es M.A.X., un conseiller IA expert en gestion de leads et CRM.

**Mode CONSEIL** : Tu donnes UNIQUEMENT des conseils, analyses et explications. Aucune action, aucune recommandation d'action. Pure analyse et conseil.

Fonctionnalit√©s disponibles :
- Analyse de fichiers upload√©s
- R√©ponses aux questions sur les leads
- Explications sur les meilleures pratiques
- Aide √† la compr√©hension des donn√©es

Tu DOIS :
- √ätre tr√®s concis et p√©dagogique
- Expliquer clairement les concepts
- Donner des exemples si pertinent
- Formater les r√©ponses en markdown

Tu NE DOIS PAS :
- Proposer d'actions ou de recommandations d'actions
- Utiliser un ton directif
- Inventer des donn√©es`
};

/**
 * R√©cup√©rer les informations tenant depuis Supabase pour enrichir le contexte
 * Utilise un fallback local si Supabase n'est pas disponible
 */
async function getTenantContext(tenantId) {
  if (!tenantId) return null;

  const context = {};

  try {
    // Essayer d'abord Supabase
    const [nom, projet, businessModel, secteur] = await Promise.all([
      getTenantMemory(tenantId, 'user_name', 'global'),
      getTenantMemory(tenantId, 'project_name', 'global'),
      getTenantMemory(tenantId, 'business_model', 'global'),
      getTenantMemory(tenantId, 'secteur', 'global')
    ]);

    if (nom?.ok && nom.data) {
      context.userName = nom.data.memory_value;
    }

    if (projet?.ok && projet.data) {
      context.projectName = projet.data.memory_value;
    }

    if (businessModel?.ok && businessModel.data) {
      context.businessModel = businessModel.data.memory_value;
    }

    if (secteur?.ok && secteur.data) {
      context.secteur = secteur.data.memory_value;
    }

    // Si on a r√©cup√©r√© des donn√©es, les retourner
    if (Object.keys(context).length > 0) {
      console.log('[OPENAI_CLIENT] üß† Contexte r√©cup√©r√© depuis Supabase');
      return context;
    }
  } catch (error) {
    console.warn('[OPENAI_CLIENT] ‚ö†Ô∏è Supabase indisponible, utilisation m√©moire locale');
  }

  // Fallback: utiliser la m√©moire locale
  try {
    const userName = getLocalMemory(tenantId, 'user_name');
    const projectName = getLocalMemory(tenantId, 'project_name');
    const businessModel = getLocalMemory(tenantId, 'business_model');
    const secteur = getLocalMemory(tenantId, 'secteur');

    if (userName) context.userName = userName;
    if (projectName) context.projectName = projectName;
    if (businessModel) context.businessModel = businessModel;
    if (secteur) context.secteur = secteur;

    if (Object.keys(context).length > 0) {
      console.log('[OPENAI_CLIENT] üíæ Contexte r√©cup√©r√© depuis m√©moire locale (fallback)');
      return context;
    }
  } catch (error) {
    console.error('[OPENAI_CLIENT] ‚ùå Erreur r√©cup√©ration m√©moire locale:', error);
  }

  return null;
}

/**
 * Enrichir le prompt syst√®me avec les informations du tenant
 */
function enrichSystemPromptWithContext(basePrompt, context) {
  if (!context) return basePrompt;

  let enrichedPrompt = basePrompt;

  // Ajouter les informations personnelles en haut du prompt
  const contextLines = [];

  if (context.userName) {
    contextLines.push(`**NOM DE L'UTILISATEUR**: ${context.userName}`);
  }

  if (context.projectName) {
    contextLines.push(`**PROJET**: ${context.projectName}`);
  }

  if (context.businessModel) {
    contextLines.push(`**MOD√àLE D'AFFAIRES**: ${context.businessModel}`);
  }

  if (context.secteur) {
    contextLines.push(`**SECTEUR**: ${context.secteur}`);
  }

  if (contextLines.length > 0) {
    const contextBlock = `
=== CONTEXTE UTILISATEUR (M√âMOIRE LONG TERME) ===
${contextLines.join('\n')}
================================================

`;
    enrichedPrompt = contextBlock + basePrompt;
  }

  return enrichedPrompt;
}

/**
 * Appel OpenAI simple (non-streaming)
 *
 * @param {Object} params
 * @param {Array} params.messages - Tableau de messages [{role: 'user'|'assistant'|'system', content: string}]
 * @param {string} params.mode - Mode M.A.X. : 'auto', 'assist', 'conseil'
 * @param {string} params.tenantId - ID du tenant (pour r√©cup√©rer contexte Supabase)
 * @param {boolean} params.useComplexModel - Utiliser GPT-4o au lieu de GPT-4o-mini
 * @param {number} params.maxTokens - Nombre max de tokens en r√©ponse (d√©faut: 2000)
 * @param {number} params.temperature - Temp√©rature (0-2, d√©faut: 0.7)
 * @returns {Promise<{content: string, usage: object}>}
 */
export async function callOpenAI({
  messages,
  mode = 'assist',
  tenantId = null,
  useComplexModel = false,
  maxTokens = 2000,
  temperature = 0.7
}) {
  try {
    const model = useComplexModel ? MODEL_COMPLEX : MODEL_SIMPLE;
    let systemPrompt = SYSTEM_PROMPTS[mode] || SYSTEM_PROMPTS.assist;

    // Enrichir le prompt syst√®me avec les informations du tenant depuis Supabase
    if (tenantId) {
      const tenantContext = await getTenantContext(tenantId);
      if (tenantContext) {
        systemPrompt = enrichSystemPromptWithContext(systemPrompt, tenantContext);
        console.log('[OPENAI_CLIENT] üß† Contexte tenant ajout√©:', {
          tenantId,
          hasUserName: !!tenantContext.userName,
          hasProjectName: !!tenantContext.projectName,
          hasBusinessModel: !!tenantContext.businessModel
        });
      }
    }

    console.log('[OPENAI_CLIENT] ü§ñ Appel OpenAI:', {
      model,
      mode,
      messagesCount: messages.length,
      maxTokens,
      temperature,
      tenantId: tenantId || 'none'
    });

    // Construire les messages avec le system prompt
    const fullMessages = [
      { role: 'system', content: systemPrompt },
      ...messages
    ];

    const completion = await openai.chat.completions.create({
      model,
      messages: fullMessages,
      max_tokens: maxTokens,
      temperature,
      top_p: 1,
      frequency_penalty: 0,
      presence_penalty: 0
    });

    const content = completion.choices[0]?.message?.content || '';
    const usage = completion.usage || {};

    console.log('[OPENAI_CLIENT] ‚úÖ R√©ponse re√ßue:', {
      contentLength: content.length,
      promptTokens: usage.prompt_tokens,
      completionTokens: usage.completion_tokens,
      totalTokens: usage.total_tokens
    });

    return {
      content,
      usage: {
        promptTokens: usage.prompt_tokens || 0,
        completionTokens: usage.completion_tokens || 0,
        totalTokens: usage.total_tokens || 0
      }
    };
  } catch (error) {
    console.error('[OPENAI_CLIENT] ‚ùå Erreur:', error.message);

    if (error.status === 429) {
      throw new Error('Limite de taux OpenAI atteinte. Veuillez r√©essayer dans quelques instants.');
    } else if (error.status === 401) {
      throw new Error('Cl√© API OpenAI invalide.');
    } else if (error.status === 500) {
      throw new Error('Erreur serveur OpenAI. Veuillez r√©essayer.');
    } else {
      throw new Error(`Erreur OpenAI: ${error.message}`);
    }
  }
}

/**
 * Appel OpenAI avec streaming (SSE)
 *
 * @param {Object} params
 * @param {Array} params.messages - Tableau de messages
 * @param {string} params.mode - Mode M.A.X. : 'auto', 'assist', 'conseil'
 * @param {boolean} params.useComplexModel - Utiliser GPT-4o au lieu de GPT-4o-mini
 * @param {number} params.maxTokens - Nombre max de tokens en r√©ponse
 * @param {number} params.temperature - Temp√©rature (0-2)
 * @returns {Promise<AsyncIterable>} - Stream de chunks
 */
export async function callOpenAIStream({
  messages,
  mode = 'assist',
  tenantId = null,
  useComplexModel = false,
  maxTokens = 2000,
  temperature = 0.7
}) {
  try {
    const model = useComplexModel ? MODEL_COMPLEX : MODEL_SIMPLE;
    let systemPrompt = SYSTEM_PROMPTS[mode] || SYSTEM_PROMPTS.assist;

    // Enrichir le prompt syst√®me avec les informations du tenant depuis Supabase
    if (tenantId) {
      const tenantContext = await getTenantContext(tenantId);
      if (tenantContext) {
        systemPrompt = enrichSystemPromptWithContext(systemPrompt, tenantContext);
        console.log('[OPENAI_CLIENT] üß† Contexte tenant ajout√© (streaming):', {
          tenantId,
          hasUserName: !!tenantContext.userName,
          hasProjectName: !!tenantContext.projectName
        });
      }
    }

    console.log('[OPENAI_CLIENT] üåä Appel OpenAI STREAMING:', {
      model,
      mode,
      messagesCount: messages.length,
      tenantId: tenantId || 'none'
    });

    // Construire les messages avec le system prompt
    const fullMessages = [
      { role: 'system', content: systemPrompt },
      ...messages
    ];

    const stream = await openai.chat.completions.create({
      model,
      messages: fullMessages,
      max_tokens: maxTokens,
      temperature,
      stream: true,
      top_p: 1,
      frequency_penalty: 0,
      presence_penalty: 0
    });

    return stream;
  } catch (error) {
    console.error('[OPENAI_CLIENT] ‚ùå Erreur streaming:', error.message);

    if (error.status === 429) {
      throw new Error('Limite de taux OpenAI atteinte.');
    } else if (error.status === 401) {
      throw new Error('Cl√© API OpenAI invalide.');
    } else {
      throw new Error(`Erreur OpenAI streaming: ${error.message}`);
    }
  }
}

/**
 * Obtenir le nom du mod√®le utilis√©
 */
export function getModelName(useComplexModel = false) {
  return useComplexModel ? MODEL_COMPLEX : MODEL_SIMPLE;
}

/**
 * V√©rifier la configuration OpenAI
 */
export function checkConfiguration() {
  return {
    configured: !!OPENAI_API_KEY,
    modelSimple: MODEL_SIMPLE,
    modelComplex: MODEL_COMPLEX,
    maxRetries: MAX_RETRY_ATTEMPTS,
    timeout: REQUEST_TIMEOUT_MS
  };
}

console.log('[OPENAI_CLIENT] ‚úÖ Client OpenAI initialis√©:', {
  modelSimple: MODEL_SIMPLE,
  modelComplex: MODEL_COMPLEX,
  timeout: `${REQUEST_TIMEOUT_MS}ms`,
  maxRetries: MAX_RETRY_ATTEMPTS
});

export default {
  callOpenAI,
  callOpenAIStream,
  getModelName,
  checkConfiguration
};
